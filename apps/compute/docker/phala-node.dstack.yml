# Babylon Compute Node - Phala Cloud Deployment
#
# Deploy with: dstack apply -f docker/phala-node.dstack.yml
#
# Required environment variables:
#   - PRIVATE_KEY: Provider's wallet private key
#   - REGISTRY_ADDRESS: ComputeRegistry contract address
#   - LEDGER_ADDRESS: LedgerManager contract address  
#   - INFERENCE_ADDRESS: InferenceServing contract address
#
# Optional:
#   - MODEL_NAME: Model to serve (default: llama2)
#   - MODEL_BACKEND: Backend type (ollama, mock)
#   - OLLAMA_ENDPOINT: Ollama server URL if using ollama backend

type: service
name: babylon-inference-node

# GPU configuration for H200
resources:
  gpu:
    name: H200
    count: 1
  memory: 64GB
  
# TEE configuration (Phala Confidential Computing)
confidential:
  enabled: true
  attestation: true

# Container image
image: ghcr.io/babylon/compute-node:latest

# Alternatively, build from source
# build:
#   context: .
#   dockerfile: docker/Dockerfile

# Port mapping
port: 8080

# Environment variables
env:
  # Required - Provider identity
  PRIVATE_KEY: ${PRIVATE_KEY}
  
  # Required - Contract addresses (Base Sepolia)
  REGISTRY_ADDRESS: ${REGISTRY_ADDRESS:-0xe7f1725E7734CE288F8367e1Bb143E90bb3F0512}
  LEDGER_ADDRESS: ${LEDGER_ADDRESS:-0x9fE46736679d2D9a65F0992F2272dE9f3c7fa6e0}
  INFERENCE_ADDRESS: ${INFERENCE_ADDRESS:-0xCf7Ed3AccA5a467e9e704C703E8D87F634fB0Fc9}
  
  # Network configuration
  RPC_URL: ${RPC_URL:-https://sepolia.base.org}
  PORT: "8080"
  
  # Model configuration
  MODEL_NAME: ${MODEL_NAME:-llama2}
  MODEL_BACKEND: ${MODEL_BACKEND:-ollama}
  OLLAMA_ENDPOINT: ${OLLAMA_ENDPOINT:-http://localhost:11434}
  
  # Pricing (in wei per token)
  PRICE_PER_INPUT_TOKEN: ${PRICE_PER_INPUT_TOKEN:-1000000000}
  PRICE_PER_OUTPUT_TOKEN: ${PRICE_PER_OUTPUT_TOKEN:-2000000000}
  
  # Security
  REQUIRE_AUTH: "true"

# Health check
health_check:
  path: /health
  interval: 30s
  timeout: 10s

# Scaling (for production)
# replicas: 
#   min: 1
#   max: 10
#   target_utilization: 80

# Gateway configuration for public access
gateway:
  # Auto-assigned domain: <service-name>.<project>.phala.network
  # Or specify custom domain:
  # domain: inference.babylon.network
  https: true

# Commands to run after deployment
commands:
  # Register with Babylon network after startup
  post_start: |
    echo "Waiting for service to be ready..."
    sleep 10
    
    # Verify health
    curl -f http://localhost:8080/health || exit 1
    
    echo "Node is ready!"
    echo "Endpoint: https://${DSTACK_SERVICE_URL}"
    echo "Provider: $(curl -s http://localhost:8080/health | jq -r .provider)"

# Volumes for model caching (optional)
# volumes:
#   - name: model-cache
#     path: /root/.cache/huggingface
#     size: 100GB

# Spot instances for cost savings (optional)
# spot_policy: auto


